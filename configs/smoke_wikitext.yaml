seed: 123
output_dir: outputs/smoke
report_file: outputs/smoke/report.json
table_file: outputs/smoke/summary.tsv
num_canaries: 5
canary_length: 12
canary_prefix: CANARY

dataset:
  name: wikitext
  subset: wikitext-2-raw-v1
  split: train
  text_field: text
  max_documents: 200
  streaming: false
  train_fraction: 0.9
  val_fraction: 0.05
  test_fraction: 0.05
  shuffle_seed: 17
  shuffle_buffer: 1000
  prefetch:
    enabled: true
    split: train
    force_download: false
dedup:
  chunk_tokens: 64
  stride_tokens: 64
  min_chunk_tokens: 32
  lowercase: true
  strip_html: true
  collapse_whitespace: true
  cross_split: true
  near:
    enabled: true
    shingle_size: 5
    num_permutations: 64
    band_size: 4
    threshold: 0.8
    skip_short_tokens: 50

training:
  block_size: 64
  n_layer: 2
  n_head: 2
  n_embd: 128
  dropout: 0.1
  lr: 0.0002
  weight_decay: 0.01
  betas: [0.9, 0.99]
  grad_clip: 1.0
  batch_size: 8
  max_steps: 30
  warmup_steps: 5
  log_interval: 10
  device: cpu
  precision: fp32
  max_train_tokens: 40000
  save_checkpoints: false

evaluation:
  val_batch_size: 8
  max_val_tokens: 32768
  downstream_tasks:
    lambada_openai: 10
    piqa: 10
    hellaswag: 10
  canary_prompts: 10
  canary_decode_max_tokens: 16

runs:
  - name: no_dedup
    apply_exact: false
    apply_near: false
    description: Raw chunks
  - name: exact
    apply_exact: true
    apply_near: false
    description: Exact hash dedup
